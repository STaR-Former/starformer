model:
  d_model: 128
  activation: selu # gelu #'silu'
  dim_feedforward: 8
  dropout: 0.14397441297939667
  n_head: 8
  num_encoder_layers: 7 
  mask_threshold: 0.0057012925042794205
  mask_region_bound: 0.25
  ratio_highest_attention: 0.2
  activation_masking: selu
  activation_cls: tanh
  output_head:
    reduced: false
optimizer:
  beta1: 0.8064811728306766
  beta2: 0.9395421940286746
  weight_decay: 0.00045761780206462983
training:
  learning_rate: 0.0017981201940106568
  batch_size: 32
loss:
  lambda_contrastive: 0.3794891956696243
  
