model:
  d_model: 128
  activation: tanh # gelu #'silu'
  dim_feedforward: 32
  dropout: 0.14397441297939667
  n_head: 4
  num_encoder_layers: 1 
  mask_threshold: 0.1
  mask_region_bound: 0.05
  ratio_highest_attention: 0.3
  activation_masking: silu
  activation_cls: relu
optimizer:
  beta1: 0.9217865818744028
  beta2: 0.9198031806475993
  weight_decay: 0.0002526374136673077
training:
  learning_rate: 0.001696617333030983
  batch_size: 32
loss:
  lambda_drm: 0.6286047112038831
  lambda_masked: 0.7335119897175937
