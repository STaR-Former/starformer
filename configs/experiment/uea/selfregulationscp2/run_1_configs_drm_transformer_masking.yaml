model:
  d_model: 32 #128
  activation: gelu #relu # gelu #'silu'
  dim_feedforward: 64 #32
  dropout: 0.09787266721910126 #0.08682056071805268
  n_head: 2 #4
  num_encoder_layers: 2 #2 
  mask_threshold: 0.043337075490129384
  mask_region_bound: 0.05
  ratio_highest_attention: 0.3 #0.1
optimizer:
  beta1: 0.8590184672150268 #0.9217865818744028
  beta2: 0.965369787236886 #0.9198031806475993
  weight_decay: 0.0002548754962105191 #0.0002526374136673077
training:
  learning_rate: 0.001696617333030983
  batch_size: 32
loss:
  lambda_drm: 0.550859659116841 #0.6286047112038831
  lambda_masked: 0.5843388142766683 #0.7335119897175937
