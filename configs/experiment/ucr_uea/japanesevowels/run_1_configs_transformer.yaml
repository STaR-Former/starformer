model:
  d_model: 32 #8
  activation: selu #'silu'
  dim_feedforward: 16 #8
  dropout: 0.010131391128967304
  n_head: 8
  num_encoder_layers: 2 #4
  output_head:
    reduced: True
optimizer:
  beta1: 0.9044308164122468
  beta2: 0.9727354041172336
  weight_decay: 0.00016095362991484007
training:
  learning_rate: 0.006566344616340228
  batch_size: 32 #16
