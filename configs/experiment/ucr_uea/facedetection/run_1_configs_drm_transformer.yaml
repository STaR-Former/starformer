model:
  d_model: 8 #128
  activation: gelu #'silu'
  dim_feedforward: 32
  dropout: 0.1073617781434152
  n_head: 4
  num_encoder_layers: 2 #2 
optimizer:
  beta1: 0.8617949433117824
  beta2: 0.9290826798040532
  weight_decay: 0.0002677700972589832
training:
  learning_rate: 0.0015574486594428344
  batch_size: 32 #16
