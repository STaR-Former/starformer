model:
  activation: 'selu' #silu'
  d_model: 8
  dim_feedforward: 8
  dropout: 0.18051489434056545
  n_head: 8
  num_encoder_layers: 4
  mask_p: 0.2 
optimizer:
  beta1: 0.9060663506326628
  beta2: 0.9452503135948176
  weight_decay: 4.745164097538667e-05
training:
  learning_rate: 0.009630287908495886
loss:
  weight: 1.0 