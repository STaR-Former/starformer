model:
  d_model: 32 #8
  activation: selu # 'gelu' 'silu'
  dim_feedforward: 16 #32
  dropout: 0.010131391128967304 #0.1073617781434152
  n_head: 8 #4
  num_encoder_layers: 2 #2 4
  output_head:
    reduced: True
optimizer:
  beta1: 0.9044308164122468 # 0.8617949433117824
  beta2: 0.9727354041172336 # 0.9290826798040532
  weight_decay: 0.00016095362991484007 #0.0002677700972589832
training:
  learning_rate: 0.006566344616340228 # 0.0015574486594428344
  batch_size: 32 #16
