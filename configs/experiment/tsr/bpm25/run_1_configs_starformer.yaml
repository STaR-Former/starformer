model:
  sequence_model:
    activation: relu 
    d_model: 16 
    dim_feedforward: 16
    dropout: 0.3 
    n_head: 2
    num_encoder_layers: 2
    mask_threshold: 0.1 # 0.2 (mask_p)
    mask_region_bound: 0.05 # 0.1
    ratio_highest_attention: 0.5 # 0.5
  output_head:
    reduced: True
    activation: relu
  text_model:
    name: null
    hidden_size: null
    aligner:
      kernel_size: null
      activation: null
optimizer:
  beta1: 0.9
  beta2: 0.999 
training:
  learning_rate: 0.001
  batch_size: 512
callbacks:
  early_stop:
    patience: 100
loss:
  lambda_cl: 1000.