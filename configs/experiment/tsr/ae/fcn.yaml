# @package _global_
dataset: appliancesenergy
task: regression

defaults:
  - override /callbacks/lr_scheduler: reduceLROnPlateau
  - override /callbacks/early_stop: tsr
  - override /callbacks/model_ckpt: tsr
  - override /datamodule: tsr/ae/centralized
  - override /loss: mean_squarred_error
  - override /model: fcn
  - override /optimizer: adam
  - override /training: tsr/centralized
  - override /logger: wandb

callbacks:
  early_stop:
    monitor: val/loss_task
    patience: 100
  lr_scheduler: 
    monitor: val/loss_task
    apply: True
    mode: min
    factor: 0.5
    patience: 50
    min_lr: 0.0001
logger:
  project: appliancesenergy
loss:
  pred_type: binary
  task: ${task}
  task_loss_fn: mean_absolute_error
model:
  sequence_model:
    input_size: 24 
  output_head:
    task: ${task}
    batch_size: ${training.batch_size}
    d_out: 1
    d_hidden: null
    activation: relu
    reduced: True 
    cls_method: pooling
optimizer:
  name: adam
  beta1: 0.9
  beta2: 0.999
  eps: 1e-07
  weight_decay: 0
training:
  epochs: 2000
  learning_rate: 0.001

