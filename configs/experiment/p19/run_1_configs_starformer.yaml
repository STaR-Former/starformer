model:
  sequence_model:
    activation: relu # 'gelu' 'silu'
    d_model: 32 #8
    dim_feedforward: 32 #32
    dropout: 0.3 #0.1073617781434152
    n_head: 2 #4
    num_encoder_layers: 2 #2 4
    mask_threshold: 0.05 # 0.2 (mask_p)
    mask_region_bound: 0.05 # 0.1
    ratio_highest_attention: 0.2 # 0.5
  output_head:
    reduced: False
    activation: selu
  text_model:
    name: roberta
    hidden_size: 768
    aligner:
      kernel_size: 3
      activation: relu

datamodule:
  upsample_percentage: 0.5
optimizer:
  beta1: 0.9
  beta2: 0.999 
  #weight_decay: 0.0 #0.0002677700972589832
training:
  learning_rate: 0.005 # 0.0015574486594428344
  batch_size: 256