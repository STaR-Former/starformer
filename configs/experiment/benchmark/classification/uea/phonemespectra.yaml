# @package _global_
seed: 42
dataset: PhonemeSpectra
task: classification
run: 2025-05-28_19:19:25
callbacks:
  early_stop:
    monitor: val/loss_ce
    mode: min
    patience: 30
    verbose: true
  model_ckpt:
    monitor: val/acc
    verbose: false
    save_last: true
    mode: max
  lr_scheduler:
    name: ReduceLROnPlateau
    apply: true
    mode: min
    factor: 0.8
    patience: 15
    min_lr: 1.0e-06
    monitor: val/loss_ce
  ema:
    apply: true
    decay: 0.999
  lr_scheudler:
    monitor: val/loss_ce
    patience: 30
datamodule:
  dataset: PhonemeSpectra
  batch_size: 128
  num_workers: 0
  seed: 42
  training_method: centralized
  max_trajectory_length: null
  aws_profile: null
  s3_bucket_path: null
  use_threads: null
  val_batch_size: 128
  test_batch_size: 128
  num_train: null
  num_val: null
  num_test: null
  text_labels:
  - 0
  - 1
  - 2
  - 3
  - 4
  - 5
  - 6
  - 7
  - 8
  - 9
  - 10
  - 11
  - 12
  - 13
  - 14
  - 15
  - 16
  - 17
  - 18
  - 19
  - 20
  - 21
  - 22
  - 23
  - 24
  - 25
  - 26
  - 27
  - 28
  - 29
  - 30
  - 31
  - 32
  - 33
  - 34
  - 35
  - 36
  - 37
  - 38
  display_labels:
  - aa
  - ae
  - ah
  - ao
  - aw
  - ay
  - b
  - ch
  - d
  - dh
  - eh
  - er
  - ey
  - f
  - g
  - hh
  - ih
  - iy
  - jh
  - k
  - l
  - m
  - n
  - ng
  - ow
  - oy
  - p
  - r
  - s
  - sh
  - t
  - th
  - uh
  - uw
  - v
  - w
  - y
  - z
  - zh
  dataset_instance: 2025-05-26_01:12:01
loss:
  loss_fn: darem_sscl
  method: sscl
  lambda_cl: 0.630085964761614
  lambda_fuse_cl: 0.5
  temp: 0.5
  batch_size: 16
  pred_type: multiclass
  task: classification
  task_loss_fn: mean_squarred_error
optimizer:
  name: adam
  beta1: 0.8237291885697735
  beta2: 0.9865949518854816
  eps: 1.0e-08
  weight_decay: 0.0004273018497850949
model:
  sequence_model:
    name: starformer
    embedding:
      d_features: 11
      max_seq_len: 218
    d_model: 128
    n_head: 8
    num_encoder_layers: 3
    dim_feedforward: 16
    dropout: 0.07083605123567172
    activation: relu
    layer_norm_eps: 1.0e-05
    batch_first: false
    bias: true
    enable_nested_tensor: true
    mask_check: true
    masking: darem
    mask_threshold: 0.3
    mask_region_bound: 0.25
    ratio_highest_attention: 0.3
    aggregate_attn_per_batch: false
    precision: 32
    reconstruction: false
    task: classification
    batch_size: 16
    cls_method: cls_token
  text_model:
    aligner:
      kernel_size: null
      activation: null
  output_head:
    task: classification
    batch_size: 128
    d_out: 39
    d_hidden: null
    activation: selu
    reduced: false
    cls_method: cls_token
system:
  accelerator: gpu
  devices: 1
  num_workers: 0
training:
  epochs: 500
  batch_size: 128
  val_batch_size: 128
  test_batch_size: 128
  learning_rate: 0.004028323632382506
  fast_dev: false
  precision: 32
  devices: null
  multi_gpu_training: false
logger:
  name: wandb # tensorboard (to use tensorboard change name)
  sweep: false 
  entity: null # add your wandb entity here
  exp_name: null
  project: starformer-reruns
  run_1_config_path: null
  sweep_id: null