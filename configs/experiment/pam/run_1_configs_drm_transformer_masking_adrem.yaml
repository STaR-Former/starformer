model:
  d_model: 64 #8
  activation: gelu # 'gelu' 'silu'
  activation_cls: silu # 'gelu' 'silu'
  activation_masking: tanh # 'gelu' 'silu'
  dim_feedforward: 32 #32
  dropout: 0.19510392696338688 #0.1073617781434152
  n_head: 2 #4
  num_encoder_layers: 1 #2 4
  mask_threshold: 0.33567807205444317
  mask_region_bound: 0.15
  ratio_highest_attention: 0.4 #0.1
  output_head:
    reduced: true
optimizer:
  beta1: 0.8268652897334431 # 0.8617949433117824
  beta2: 0.9345730561683292 # 0.9290826798040532
  weight_decay: 0.00015973734428624664 #0.0002677700972589832
training:
  learning_rate: 0.0059920239953448655 # 0.0015574486594428344
  batch_size: 64 #16
loss:
  lambda_contrastive: 0.6950129072155029

