model:
  d_model: 32 #16
  activation: 'gelu' #'silu'
  dim_feedforward: 64 #16
  dropout: 0.2707381623498213 # 0.0
  n_head: 2
  num_encoder_layers: 6 #2 
optimizer:
  beta1: 0.8982056936978817 #0.99
  beta2: 0.9979217684625786 #0.999
  weight_decay: 0.000408482994884476 #0.0
training:
  learning_rate: 0.001577221061718158 # 0.005
  batch_size: 32 #16
