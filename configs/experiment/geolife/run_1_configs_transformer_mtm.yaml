model:
  activation: 'silu'
  d_model: 16
  dim_feedforward: 16
  dropout: 0.0
  n_head: 4
  num_encoder_layers: 4
  mask_p: 0.2 
optimizer:
  beta1: 0.99
  beta2: 0.999
  weight_decay: 0.0
training:
  learning_rate: 0.005
loss:
  weight: 0.5 