# @package _global_
# p19
dataset: p19
task: classification

defaults:
  - override /callbacks/lr_scheduler: reduceLROnPlateau
  - override /callbacks/early_stop: p19
  - override /callbacks/model_ckpt: p19
  - override /datamodule: p19/centralized
  - override /loss: darem_sscl
  - override /model: starformer
  - override /optimizer: adam
  - override /training: p19/centralized
  - override /logger: wandb

run_url: null # wandb run url
change_config: {
  'task': classification,
  'callbacks': {
    'early_stop': {
      'mode': 'min',
      'monitor': 'val/loss_ce',
      'patience': 30,
      'verbose': true,
    },
  },
  "logger": {
    "sweep": false, 
    "project": "p19"
  },
  # LSTM 
  #'model': {
  #  'sequence_model': {
  #    'name': "lstm",
  #    'input_size': 8,
  #    'hidden_size': 16,
  #    'num_layers': 5
  #    'dropout': 0.3,
  #    'bias': True,
  #    'batch_first': False,
  #    'bidirectional': False, 
  #  },
  #  # Output head
  #  'output_head': {
  #    'task': classification 
  #    d_out: 1 
  #    d_hidden: null
  #    activation: relu
  #    reduced: True
  #    cls_method: autoregressive # options: 'cls_token', 'autoregressive', 'elementwise'
  #  }
  # transformer / starformer-rm
  "model": {
    "text_model": {
      "name": "roberta",
      "aligner": {
        "activation": "selu",
        "kernel_size": 7
      },
      "hidden_size": 768
    },
    "output_head": {
      "task": "classification",
      "d_out": 1,
      "reduced": true,
      "d_hidden": null,
      "activation": "silu",
      "batch_size": 512,
      "cls_method": "cls_token"
    },
    "sequence_model": {
      "bias": true,
      "name": "starformer",
      "n_head": 2,
      "d_model": 128,
      "dropout": 0.28195450774463493,
      'masking': random, # transformer / starformer rm adjustments
      "mask_threshold": 0.05, # 0.05,
      'mask_region_bound': 0.2, # 0.2, 
      "ratio_highest_attention":  0.3, # 0.3
      "embedding": {
        "d_features": 34,
        "max_seq_len": 61
      },
      "precision": 32,
      "activation": "gelu",
      "mask_check": true,
      "batch_first": false,
      "layer_norm_eps": 0.00001,
      "reconstruction": false,
      "dim_feedforward": 32,
      "num_encoder_layers": 5,
      "enable_nested_tensor": true,
      "aggregate_attn_per_batch": false
    }
  },
  'loss': {
    'batch_size': 512,
    'lambda_cl': 0.6300672155139967,
    'lambda_fuse_cl': 0.5,
    'loss_fn': "darem_sscl",
    'method': "sscl",
    'pred_type': "binary",
    'temp': 0.5,
    'task': '${task}',
    #'loss_fn': 'binary_cross_entropy',
  },
  'datamodule': {
    'train_split_index': 4
  }
  #"seed": 123
  #"seed": 0
  #"seed": 63
  #"seed": 2024

}
