program: scripts/training/train.py

method: bayes
metric: 
  name: test/acc
  goal: maximize
    #name: test/fbeta
    #goal: maximize
project: pam_sweeps

parameters:
  training.learning_rate:
    max: 0.01
    min: 0.00001
    distribution: uniform
  training.batch_size:
    values:
      #- 16
      - 32 
      - 64
      - 128
      - 256
      #- 512
    # - 32 # leads to memory errors
    distribution: categorical
  optimizer.beta1:
    max: 0.99     
    min: 0.8
    distribution: uniform
  optimizer.beta2: 
    max: 0.9999 
    min: 0.9
    distribution: uniform
  optimizer.weight_decay: 
    max: 0.0005
    min: 0.0
    distribution: uniform
  ## model
  model.d_model:
    values:
      - 8
      - 16
      - 32
      - 64
      - 128
    distribution: categorical
  model.n_head:
    values:
      - 2
      - 4
      - 8
    distribution: categorical
  model.num_encoder_layers:
    values:
      - 1
      - 2
      - 3
      - 4
      - 5
      - 6
      - 7
      - 8 
    distribution: categorical
  model.dim_feedforward:
    values:
      - 8
      - 16
      - 32
      - 64
      - 128
    distribution: categorical
  model.dropout:
    max: 0.3
    min: 0.0
    distribution: uniform
  model.activation:
    values:
      - elu
      - relu
      - selu
      - gelu
      - silu
      - tanh
    distribution: categorical
  model.mask_threshold:
    min: 0.0
    max: 0.5
    distribution: uniform
  model.mask_region_bound:
    values:
      - 0.05
      - 0.1
      - 0.15
      - 0.2
      - 0.25
      - 0.3
    distribution: categorical
  model.ratio_highest_attention:
    values:
      - 0.1
      - 0.2
      - 0.3
      - 0.4
      - 0.5
    distribution: categorical
  model.activation_masking:
    values:
      - elu
      - relu
      - selu
      - gelu
      - silu
      - tanh
    distribution: categorical
  model.activation_cls:
    values:
      - elu
      - relu
      - selu
      - gelu
      - silu
      - tanh
    distribution: categorical
  model.output_head.reduced:
      values:
        - true
        - false
      distribution: categorical
  # loss
  loss.lambda_contrastive:
    min: 0.1
    max: 1.0
    distribution: uniform
  #loss.lamdba_sim:
  #  values:
  #    - 0.0
  #    - 0.5
  #    - 1.0
  #  distribution: categorical


early_terminate:
  type: hyperband
  s: 2
  eta: 3
  max_iter: 27


command:
 - python
 - ${program}
 - +experiment=pam/sweep_drm_transformer.yaml
 - system.accelerator=gpu
 - model.masking='drm'
 - loss='contrastive'
 - logger.run_1_config_path='configs/experiment/pam/run_1_configs_drm_transformer_masking_adrem.yaml'
 - training.log_every_n_steps=10
